\documentclass[12pt]{article}
%\usepackage{apacite}
\usepackage{wrapfig}
\setlength{\parindent}{0pt}
\usepackage{caption} %test
\captionsetup[figure]{font=small} %test
\usepackage{tgtermes}
\usepackage{setspace}
\usepackage{gensymb}
\usepackage{amsmath}
\doublespacing
\usepackage{graphicx}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage[backend=biber,style=apa,autocite=inline]{biblatex}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Affective weighting function}
\rhead{Victor M. Poulsen, Studie Nr.: 201707639}

\DeclareLanguageMapping{english}{english-apa}
\addbibresource{References.bib}
\setcounter{page}{1}

\title{Affective modulation of the weighting function}
\author{Victor MÃ¸ller Poulsen, Studie Nr.: 201707639}

\begin{document}
\maketitle
\leavevmode

\section{Description}

Code for Simulation \& Figures available at:
https://github.com/victor-m-p/BayesianDecisionWeights

Both Expected-utility theory (EUT) and prospect theory
(PT) posit that humans maximize some version of
utility. The theories get there by a
combination of two functions
\autocite{rottenstreich2001money}.
A value function $v$ transforms objective value to
subjective utility, and a weighting function $w$
distorts probabilities \autocite{rottenstreich2001money,
gonzalez1999shape}. Expected-utility and
prospect theory combine these two paramters in
the simplest way possible
\autocite{rottenstreich2001money}

\[
	\sum w(p_i)v(i),
\]

where $p$ stands for probability and $i$ stands for the
$i^{th}$ gamble.

In EUT the weight function $w$
is the identity $w(p) = p$ assuming that people do
not distort probabilities \autocite{rottenstreich2001money}.
In expected-utility the value functin $v$ is
proposed to reflect how people feel about
end states. This assumes that people should
take into account their current state (e.g. of wealth)
when evaluating outcomes
\autocite{newell2015straight}. \\

With regards to both $v$ and  $w$
PT \autocite{
	PT,
tversky1992advances} advances theorizing,
and better reflects empirical results.
It is arguably the main model of human decision making
\autocite{newell2015straight}. \\

PT advances theorizing
with regards to $v$ by positing that losses and
gains are evaluated as changes in wealth rather
than with regards to end states. This means that
rich and poor people should show relatively similar
behaviour, because both evaluate outcomes based
on a neutral starting point
\autocite{newell2015straight}.
This leads us to the familiar (non-linear) S-shaped
value function $v$ proposed in PT \autocite{PT}.
The value function $v$ is concave for the gains
domain and convex for losses. This reflects the
fact that small changes in outcome are relatively
overweighted. I.e., a monetary increase from
$0 - 100\$$ has greater utility than an increase
from $1000 - 1100\$$. The same is the case for the
domain of losses. Another key stylistic of the
value function is that it is steeper for the
loss domain, showing loss aversion. \\

PT advances theorizing with regards to $w$
by proposing a non-linear probability
distortion \autocite{PT}. $w$ is stylized as
being reverse S-shaped, meaning that it is
concave for low probabilities and convex for
high probabilities \autocite{gonzalez1999shape}.
This means that
people underweight changes in probability in
the middle of the spectrum (e.g. $[0.2-0.8]$)
while overweighting changes in probability close
to the end-points (e.g. $[0.0 - 0.2], [0.8 - 1.0]$).
These general characteristics of the weighting
function are empirically well documented
\autocite{tversky1992advances,
wu1996curvature}.

\subsection{Prior work}

There is evidence to support
the notion that the affect of outcomes modulates
the parameters of both $v$
\autocite{hsee2004music} and $w$ \autocite{
rottenstreich2001money}. \\

The S-shape of the weighting function
$w$ appears to be more pronounced
for high-affect than low-affect outcomes under
uncertainty \autocite{rottenstreich2001money}.
They investigated how much participants were
willing to pay for two coupons
(worth the same) at different
levels of probability. The first item was
a trip to Europe (high-affect) while the second
item was tuition covering (low-affect). They
were able to show a preference reversal in
which the high-affect outcome was preferred for
low probability (1\%) whereas a low-affect
outcome was preferred for high probability (99\%).
If the results are solid, they suggest
that people distort probabilities
more for high-affect as compared to low-affect
outcomes. \\

For gains, the value function $v$ becomes more
concave for high-affect as opposed to low-affect
outcomes \autocite{hsee2004music}.
\textcite{hsee2004music} showed this by
priming participants to
evaluate outcomes either based on calculation or
based on feeling. Their results clearly suggest
a modulatory effect of affect-richness. Taken
together the results suggest a consistent picture
of modulation of both the value function $v$ and
the weight function  $w$. This line of evidence
has been pursued elsewhere \autocite{
	mukherjee2010dual,
mukherjee2011thinking} with the idea of modeling
decision making as an interaction between an
affective system and a deliberative system.

\subsection{Focus and parameterization}

In this article we focus exclusively on the
weighting function $w$ while ignoring both
the value function $v$ and the combination
of the two functions. We also restrict ourselves
to the gains domain.
In \textcite{rottenstreich2001money} they
propose that the affective modulation can
be estimated as an affect paramter $a$
in the form:

\[
	w(p) = \frac{p^{1-a}}
	{p^{1-a}+(1-p)^{1-a}}
.\]

where $a \in [0, 1]$ and larger $a$ values indicate
greater affect
and more curvature \autocite{rottenstreich2001money}.
The issue with this one-parameter
formulation is that it
does not account for the fact that people
generally show low \emph{elevation}.
What I mean by that is that the empirically
observed weighting function $w$ typically
crosses the diagonal line
at around $0.3$ rather than $0.5$
\autocite{gonzalez1999shape}. This can be
interpreted as people generally being
pessimistic (i.e. $50\%$ probability is
evaluated as being worth less than  $50\%$
of the outcome).
The one-parameter
formulation fixes this point at $0.5$,
(i.e. $w(.5) = 0.5$),
which can be seen from figure $1$.

\begin{figure}[H]
	\includegraphics[width = \linewidth]{../Figures/oneParam.png}
	\caption{Data simulated from the model
		$w(p) = \frac{p^{1-a}}
		{p^{1-a}+(1-p)^{1-a}}$ with
		$a \in [0, 1]$. Diagonal line has
		$a = 0$, and the horizontal line
		has $a = 1$. Intermediate curves
		are generated for $0.2$ increments
		of $a$. All values beside
		$a = 0$ show a probability distortion
		as compared to the objective probability.
		Note that all curves meet at
		$w(p) = 0.5, p = 0.5$. This is
	not empirically supported.}
\end{figure}

Instead of using the parameterization
proposed in \textcite{rottenstreich2001money}
this paper will use the parameterization
of $w$ proposed in \textcite{gonzalez1999shape}.
They parameterize $w$ with two parameters;
$\delta$ and $\gamma$.

\vspace{3mm}

The $\delta$ parameter will vary based on
\emph{elevation} (intercept)
\autocite{gonzalez1999shape},
which here simply refers to the overall
perceived attractiveness of outcomes
under uncertainty.

\vspace{3mm}

The $\gamma$ parameter will vary based on
\emph{curvature} (slope)
\autocite{gonzalez1999shape} and is what we
are primarily interested in for our purposes.
It follows as a direct prediction from
\textcite{rottenstreich2001money} that the
curvature ($\gamma$) should be modulated by changes in
the affective level of outcomes. \\

$p(w) = p$ for  $\gamma = 1, \delta = 1$
with this parameterization. Higher  $\delta$
corresponds to higher elevation, and
higher $\gamma$ corresponds to \emph{less}
curvature (unintuitively, and as opposed
to the $\alpha$ parameter in the one-parameter
$w$ function). See figure 2 for an
illustration of how the $\delta$
and $\gamma$ parameters independently modulate
different aspects of the weighting function $w$.

\begin{figure}[H]
	\includegraphics[width = \linewidth]{../Figures/Fig2.png}
	\caption{Data simulated from the model
		$w(p) = \frac{\delta \cdot p^{\gamma}}
	{\delta \cdot p^{\gamma} +
	(1-p)^{\gamma}}$ similarly to figure 4
	of \textcite{gonzalez1999shape}.
	On the left: $\gamma$ fixed at $0.6$
	and $\delta$ varied between $0.2$ and $1.8$.
	On the right: $\delta$ fixed at $0.6$
	and $\gamma$ varied between $0.2$ and $1.8$.
	Shows that $\gamma$  controls
	curvature and $\delta$ controls
	elevation. The identity function $w(p) = p$
	is achieved for $\delta = 1, \gamma = 1$.
	Note.. gamma low has the opposite interpretation
	as compared to rottenstreich?}
\end{figure}



\vspace{3mm}

The model proposed in \textcite{gonzalez1999shape} is:

\[
	\log\frac{w(p)}{1-w(p)} =
	\gamma \log\frac{p}{1-p} + \tau
.\]

where solving for $w(p)$ and setting $\delta = \exp(\tau)$
gives us

\[
	w(p) = \frac{\delta \cdot p^{\gamma}}
	{\delta \cdot p^{\gamma} +
	(1-p)^{\gamma}}
.\]

\subsection{Methodology}

Two studies are proposed to
properly test the robustness
of affect-level on the
curvature ($\gamma$) of the
weight function $w$.

\vspace{3mm}

In the first study, subjects will be asked to
rate the affect-richness of 10 different
items.
All outcomes
consist of coupons redeemable
for various items, all worth $\$500$.
The 10 items are designed to cover the
full spectrum from affect-rich to
affect-poor.

\vspace{3mm}

\emph{Example of expected high-affect item:} \\
"If you won a $\$500$ coupon redeemable
for a vacation abroad with a friend/partner
how emotionally
affected would you be?"

\vspace{3mm}

\emph{Example of expected low-affect item:} \\
"If you won a $\$500$ coupon redeemable
for insurance covering how emotionally
affected would you be?"

\vspace{3mm}

For the full list of items see \emph{Appendix A}.
Participants
will indicate how affect-rich
each outcome is with a slider. Participants will
see "not affected at all" (left),
"somewhat affected" (middle)
and "very affected" (right).
We will receive continuous ratings from $0$
(affect poor) to $1$ (affect rich). A mean
affect rating across participants for each
item will rank them from least affective to
most affective. Three items are
then selected: The least affective item ($A$),
the most affective item ($C$) and the item in
between these two extremes ($B$) which separate them
best (follow up). Welch two sample t-tests
between $A$ and  $B$,
and between $B$ and  $C$ will indicate whether
they differ significantly.

\vspace{3mm}

In the second study, subjects will be presented
with the three items ($A$,  $B$,  $C$)
which have been validated
for affect-richness in the prior study. All
subjects rate items in all three conditions,
making the study a within-subject design.
The formulation
around the items is that of a gamble.
The formulation is the same
for all items: \\

"You can buy a lottery ticket with an $[x]$
percent chance of winning a $\$500$ coupon
redeemable for $[y]$ with a $[1-x]$ percent
chance of winning nothing. How much are you
willing to pay for the lottery ticket?" \\

The three selected items are inserted as $[y]$
and $50$ different probability levels:
$x = 0.01, 0.03, \ldots, 0.99$ will be
inserted as $[x]$ and the negation $[1-x]$.
With all
possible combinations, this means that
all participants will rate the items of
the $3$ conditions
at $50$ different levels of certainty each.
As in experiment 1 participants will rate
with a slider. This time ranging from
$\$0$ to $\$500$ as it is neither logical
to assign a value below $\$0$ or above
$\$500$ to any of the gambles.
The approach is somewhat
different from \textcite{gonzalez1999shape}
but ultimately we estimate the same thing that
they do; participants' certainty equivalence (CE).
This simply is the amount of money they think
that the gamble is worth. \\

Note that we are not directly measuring either
$\delta$ or $\gamma$. What we do measure is the
dependent variable $w(p)$ and the independent
variables $p$ for three conditions (ranked
based on affect-richness).  \\

In order to infer the
unmeasured paramters a bayesian (non)linear
mixed effects model is proposed. The model
is fitted in $R$ \autocite{rcore}
with the  $brms$ package \autocite{brms}.
Here we can specify the previously
mentioned formula:

 \[
	 w(p) \sim \frac{\exp({\tau})\cdot p^{\gamma}}
	 {\exp({\tau})\cdot p^{\gamma}+(1-p)^{\gamma}}
.\]

It is extremely important that we specify
that we want to measure $\exp(\tau)$ rather
than  $\delta$ as this tells the models that
this parameter must be positive and thus
limits the flexibility of the model in an
appropriate way.  $\delta$ can be inferred
afterwards by taking  $\exp{\tau}$.
Additionally, we have to specify that the model should
be nonlinear - as we believe that the weighting
function is non-linear.
We can further specify that we would like to
estimate specifically the value for $\tau$
and $\gamma$ with random intercepts (partial pooling)
for participants (ID) and with item (condition)
as a main
effect.

 \[
	 \tau \sim 0 + item + (1|ID),
\]
\[
	\gamma \sim 0 + item + (1|ID)
.\]

As mentioned, estimated $\tau$ is converted
to $\delta$ by exponentiating the estimated
$\tau$ value after model fitting. \\

Results are reported as $.66$ and $.95$
credibility intervals
for the $\delta$ and $\gamma$ distributions
for each condition. Posterior samples
are drawn from the distributions, allowing
for a nice visualization of effects.

\section{Hypotheses}

\subsection{Study 1}
\emph{Hypothesis 1}: As explained earlier,
three outcomes are selected from the
10 investigated outcomes. The most
affective ($C$), the least affective ($A$) and the
question which best separate the two ($B$).
It is hypothesized that pairwise t-tests (Welch
Two Sample)
between $A$ and  $B$ and between  $B$ and  $C$
will result in significant differences.
This has to be achieved before conducting
study 2, as that study relies on this effect.
As such, if this effect is not achieved, another
study should be conducted to validate questions
before proceeding with study 2. With that said
however, it does appear reasonable that
the 10 different questions should cover the
spectrum of affect-richness pretty well
(see \emph{Appendix A}) and as such it is
expected that three items which differ significantly
can be extracted.

\subsection{Study 2}
\emph{Hypothesis 1:} A directional effect is
predicted for the $\gamma$ parameter of the function:

\[
	w(p) = \frac{\delta \cdot p^{\gamma}}
	{\delta \cdot p^{\gamma}+(1-p)^{\gamma}}
.\]

Recall that study 2 uses the three
items from study 1 as three conditions.
We will refer to these as conditions
low-affect $A$, medium-affect $B$, and
high-affect $C$. It hypothesized that
posterior credibility intervals ($.95$)
for the $\gamma$
parameter will not overlap between
conditions, and that $\gamma$ highest for
$A$, lower for  $B$ and lowest for  $C$
(recall that high affect is predicted to
result in high curvature, which is offered
by low $\gamma$). This effect would replicate
and seriously strengthen the results of
\textcite{rottenstreich2001money}. A
weaker replication would consist of
credibility intervals ($.66$) showing the
same effect. This would still be an
interesting result, but would indicate a
weaker and less reliable effect. \\

\emph{Hypothesis 2}: Additionally,
\textcite{gonzalez1999shape} report
population $\gamma = 0.44$ (median).
They use monetary gambles (low-affect),
and as such it is hypothesized that
$\gamma = 0.44$ will be within the  $.95$
credibility interval of our low-affect
condition ($A$). This would serve as
a replication of the findings of
\textcite{gonzalez1999shape}. Again, a
less convincing, but interesting result would
be to observe $\gamma = 0.44$ within the
 $0.66$ credibility interval. Minimally
interesting effects for $\gamma$ are
shown in figure 3, where  $\gamma$
values are  $A = 0.44, B = 0.34, C = 0.24$.

\begin{figure}[H]
	\includegraphics[width = \linewidth]{../Figures/ourHyp.png}
	\caption{Three curves shown, all with $\delta = 0.77$
	as reported in \textcite{gonzalez1999shape}.
	$\gamma$ levels $0.24, 0.34, 0.44$. The
	least curved line corresponds to $\gamma = 0.44$,
	as reported in \textcite{gonzalez1999shape}.
	For high-affect items $\gamma$ should
	be lower, and as such we suggest $0.24$
	(for high affect) and  $0.34$ (for medium
	affect) as minimally interesting effects
to detect}.
\end{figure}

\emph{Hypothesis 3:} No direction of effect
for the $\delta$ parameter by condition is
hypothesized. The  $\delta$ parameter is
not of interest to the main hypothesis
(replicating and extending \textcite{rottenstreich2001money})
and is mainly included in the analysis in
order to control for elevation and properly
estimate  $\gamma$. If the three items differ
in perceived overall value the $\delta$ parameter
should capture this. This means that our $\gamma$
distributions should still be interpretable
even if the $\delta$ parameter differs by
condition. The same analysis pipeline will be applied
to $\delta$ as for $\gamma$ (i.e. credibility
intervals estimated) but as suggested,
it is not clear whether an effect would
be interesting. The $\delta$ parameter is
expected to have a value close to $.77$
which is the population median found for
this parameter is \textcite{gonzalez1999shape}.

\subsection{Simulation}

In order to test the pipeline for the
bayesian analysis, data simulation was
conducted. Unfortunately, \textcite{gonzalez1999shape}
does not exactly report the values (i.e.
distributional properties of $\tau$ and
$\gamma$) that we need
to generate data consistent with what they
gathered. As such, it does not make sense
to calculate power based on our simulations,
and the simulation serves only the purpose
of making clear how analysis on eventual data
will be conducted. \\

Data is generated for $50$ probability levels,
$p = 0.01, 0.03,  \ldots, 0.99$ crossed with $3$
conditions, corresponding to the actual data
that will be collected.
Data is generated for $30$ simulated subjects (ID). \\

Note that standard deviations vary
between $\gamma$ and $\delta$, and
between population level and individual
variation. This qualitatively
follows the results of \textcite{gonzalez1999shape}.
Data is generated as a distribution of $\gamma$
and $\delta$ for each condition. We generate
$30$ values (i) for each, corresponding to the
number of participants. As we do not hypothesize
that $\delta$ is modulated by condition
this can simply be generated as once.


\begin{equation} \label{eq1}
\begin{split}
	\gamma_{A_{i}} &\sim norm(n = 30,
	m = 0.24, sd = 0.1) \\
	\gamma_{B_{i}} &\sim norm(n = 30,
	m = 0.34, sd = 0.1) \\
	\gamma_{C_{i}} &\sim norm(n = 30,
	m = 0.44, sd = 0.1) \\
	\delta_i &\sim norm(n = 90,
	m = 0.77, sd = 0.2)
\end{split}
\end{equation}

Based on these $\gamma$ and $\delta$ values for
participants per condition, we generate
the final $\gamma$ and $\delta$ values by
adding individual noise for each probability
level (j)

\begin{equation} \label{eq2}
\begin{split}
	\gamma_{A_{ij}} &\sim norm(n = 50,
	m = \gamma_{A_{i}}, sd = 0.1) \\
	\gamma_{B_{ij}} &\sim norm(n = 50,
	m = \gamma_{B_{i}}, sd = 0.1) \\
	\gamma_{C_{ij}} &\sim norm(n = 50,
	m = \gamma_{C_{i}}, sd = 0.1) \\
	\delta_{ij} &\sim norm(n = 150,
	m = \delta_{i}, sd = 0.3)
\end{split}
\end{equation}

As such, each condition will contain
two levels of noise around a true signal.
The simulated data, and the best fit
$w(p)$ curves are shown in figure 4.
As can be seen, the simulated data shows
the expected pattern, where low values of
$\gamma$ exhibit more curvature. The preference
reversal shown in \textcite{rottenstreich2001money}
is also seen in the plot.

\begin{figure}[H]
	\includegraphics[width = \linewidth]{../Figures/simulated.png}
	\caption{Plot of simulated data in three
		conditions ($A: \gamma = 0.24$,
		$B: \gamma = 0.34$,
		$C: \gamma = 0.44$). In all conditions
		the true population mean of
		$\delta = 0.77$. Shows the preference
		reversal observed in
	\textcite{rottenstreich2001money}. Note
	that the yellow curve corresponds
	roughly to what was found in
	\textcite{gonzalez1999shape}.
	The population effect is a
	weak, but true signal, which is
what we expect from the real data.}
\end{figure}

Next, the model described earlier is fitted
to the data. Regularizing riors are specified:

\[
	\tau \sim normal(0, 1)
\]

\[
	\gamma \sim normal(0.3, 0.5)
\]

Reflecting our knowledge of reasonable values for
these parameters. The same priors will be used for
modeling the actual data. Various characteristics,
such as R-hat and pp\_checks (from \emph{brms})
indicate a good model fit. \\

\begin{figure}[H]
	\includegraphics[width = \linewidth]{../Figures/gamma.png}
	\caption{showing the estimated  $\gamma$
	population distributions. The thick
	black line is  $.66$ credibility intervals
	whereas the thin black line is  $0.95$
	credibility intervals. Note that the
	conditions are ordered as expected with
	 $A$ having the lowest  $\gamma$ and
	 $C$ having the highest  $\gamma$.
	 Note that the  $0.95$ credibility
	 intervals do not overlap.}
 \end{figure}

We extract credibility intervals with regards
to $\gamma$ and $\delta$ distributions for each
condition ($A$, $B$, $C$). With the simulated
data, we note that the model is capable of
recovering these unobserved (unmeasured) parameters,
and that for $\gamma$ the  $.95$ credibility
intervals do not overlap between conditions
(see figure 5). The estimates and credibility intervals
are, $\gamma_{A} = 0.37, \: CI: [0.34, 0.40]$,
$\gamma_{B} = 0.30, \: CI: [0.27, 0.33]$ and
$\gamma_{C} = 0.20, \: CI: [0.17, 0.23]$.
This slightly underestimates the true effect, which
we know because we simulated the data. However, it
is reasonably close. This shows that with $30$
participants, and 50 probability levels crossed with
3 conditions it is possible to detect the effect
that we specified. This of course assumes specific
distributional characteristics and noise-levels
that we cannot know in advance. It does however, show
that the model works as intended.

 \section{Design Plan}

\textbf{Study type:} Study 1 might be characterized
as an observational study, since it does not
really have an experimental manipulation. It
resembles a survey of questions (e.g. the 10
outcomes). Study 2 is an experiment using a
within-subjects design, in which all participants
participate in all three conditions ($A$,  $B$,  $C$).
This is important because within-subject designs
have better power to detect effects than
between-subject designs
\autocite{charness2012experimental}. Power is a
primary concern because effects are likely
to be small, and variance is likely to be high
\autocite{gonzalez1999shape}. A between-subjects
design is not necessary in our case, because we do
not induce an effect by priming, as in e.g.
\textcite{hsee2004music}.

\textbf{Blinding:} No blinding is involved in this study. \\

\subsection{Study Design}

\emph{Study 1}: All subjects will rate all 10 items
(see Appendix 1) as to the level of affect they
feel with regards to them.

\emph{Study 2}: All participants
indicate their certainty equivalence (CE) for all
certainty levels $p = 0.01, 0.03, \ldots, 0.99 (n = 50)$
and in all three conditions ($A$,  $B$,  $C$).
This results in $150$ observations per participant,
and  $50$ observations per participant for each
condition.

\section{Sampling Plan}

\textbf{Existing Data}: Registration prior
to creation of data. Data from simulation
does exist.

\textbf{Data collection procedures}:
Participants will be recruited through online
channels (e.g. facebook, student groups, etc.).
Participants must be at least 18 years old to
participate. In the first experiments subjects
will be payed $30$ DKK for agreeing to participate
in an approx. $10$ minute online survey. In the
second experiment subjects will be payed $150$ DKK
for agreeing to participate in an approx. $60$ minute
online experiment

\textbf{Sample size}:

\emph{Study 1}: $30$ participants are recruited. \\

\emph{Study 2}: $30$ participants are recruited.

\textbf{Sample size rationale}:

\emph{Study 1}: Data was simulated to estimate the
approximate sample size needed for a Welch two
sample t-test to dissociate the three most
different items (questions). Plotting and common
sense was used to arrive at best guesses for
reasonable values. Three distributions were generated
(assumed normal, although that is not generally true
for slider data):

\begin{equation} \label{eq3}
\begin{split}
	item_A &\sim norm(0.2, 0.4) \\
	item_B &\sim norm(0.5, 0.4) \\
	item_C &\sim norm(0.8, 0.4)
\end{split}
\end{equation}

With $n = 30$ participants we have extremely
high power to detect these effects. However,
as this is a cheap experiment to run, and the
results are critical for the second study
(it is important that the three best items are
used as the conditions in experiment 2)
this is deemed to be reasonable.

\vspace{3mm}

\emph{Study 2}: Choice of sample
size is naturally related to
power. Typically, $.8$ power is considered reasonable
\autocite{cohen1992power},
although this is just convention. Power reflects the
ability to detect a effect and is influenced by
effect size and number of participants.
Unfortunately, a power simulation was not possible
to carry out since reasonable estimates for the
distributions (and effect sizes) are not present.
The simulation presented earlier likely underestimates
individual variation, which will lessen power.
The best comparison that we have is
\textcite{gonzalez1999shape} who estimate both
parameters of the value function $v$ as well
parameters $\gamma$ and  $\delta$ of the weighting
function  $w$. They do so with  $10$ participants and
collect data with respect to $15$ gambles crossed
with  $11$ probability levels \autocite{gonzalez1999shape}.
The fact that they are able to reasonable recover
the unobserved parameters (of  $v$ as well) with
a sample size of only $10$ participants suggests
that it is not so much the number of participants,
but rather the number of trials for each participant
that is important. More noise is observed within
participants than between \autocite{gonzalez1999shape}.
Based on their results and on the simulation carried
out, it is argued that $30$ participants should
probably give us reasonable power to detect a
minimally interesting effect.

\section{Variables}

\subsection{Manipulated variables}

\emph{Study 1}: No manipulated variables.
The study is an observational survey, where
participants use a slider to indicate the
affect-richness of unordered outcomes. \\

\emph{Study 2}: $50$ levels of uncertainty are
crossed with  $3$ conditions (different gambles).
These are the two manipulated variables of
study 2.

\subsection{Measured variables}

\emph{Study 1}: The single outcome variable
will be the rating of affect level. This will
be measured on a scale of $0-1$. Participants
will rate this using a slider (and will not see
the same scale that we measure). \\

\emph{Study 2}: The single outcome variable
is the price that subjects indicate that they
are willing to pay for a ticket in a lottery.
This measures the certainty equivalence (CE) of
participants, and can be thought of as $w(p)$.
This will be measured on a scale of $0-500$ dollars
using a slider. The max is 500 dollars since the
lottery tickets by definition cannot be worth
more than this.

\subsection{Indices}

No indices are used.

\section{Analysis Plan}

All analysis is performed in the programming
language \emph{R} \autocite{rcore} using
\emph{Rstudio} IDE
\autocite{rstudio}. A key package used for
bayesian model fitting is \emph{brms}
\autocite{brms}. \\

\emph{Study 1}: The affect ratings will be
ordered based on group-level means. The
three questions that best separate the
are validated as being statistically significant
with a Welch two sample t-test. \\

\emph{Study 2}: A bayesian generalized nonlinear
mixed effects model is fit to the data using the
\emph{R} package \emph{brms} \autocite{brms}.
This is done to estimate the unobserved parameters
$\delta$ and $\gamma$ from the independent variables
(1) probability level and (2) category,
and their relation to the dependent variable
$w(p)$ which is the observed certainty equivalence (CE).
Regularizing priors are specified for both
$\gamma$ and $\delta$ as described in the
"simulation" section.

\section{Discussion \& Future Work}

The two-part study presented is important
for several reasons. Firstly, it is part of
a movement to formalize psychology and decision
making (DM) which the author believes is both
important and generally done too rarely.
The effect that this study tries to replicate
\autocite{rottenstreich2001} is an interesting
study. However, it bases the conclusion of a
modulation of the weighting function
by affect based on a measurement of end-points
($1\%$ and $99\%$) only.
As such, it is rather stylized; i.e. they
are not able to estimate the modulation of
the actual parameters of the weighting function.
It is surprising that no-one (to the knowledge
of me) has actually followed the suggestion
of \autocite{rottenstreich2001} in extending
their stylized effect and testing it across
more than two items, and across enough uncertainty
levels to estimate parameters of the weighting
function $w$.

\vspace{3mm}
The present study also attempts to facilitate
cumulative science more generally, by providing
all code, simulation and eventual data at:
https://github.com/victor-m-p/BayesianDecisionWeights.
By making the code and data accessible future
experimenters can use the knowledge gained in
this experiment to motivate stronger priors -
effectively pooling information across studies
(by using our posteriors as priors). Sadly,
most of the research in this area is carried
out in a frequentist framework
\autocite{gonzalez1999shape, rottenstreich2001money,
hsee2004music}, and often
code and data is not accessible which makes it
impossible to properly integrate previous work
and thus properly build upon previous work.

\section{Appendix}

\emph{Appendix A} \\
As all items in study 1 follow the same template: \\

"If you won a $\$500$ coupon redeemable for/at
$[x]$ how emotionally affected would you be?" \\

The $10$ proposed $[x]$ outcomes are:
\begin{itemize}
	\item for a vacation abroad with a friend/partner.
	\item at a local shopping mall.
	\item for donation to a charity of your choice.
	\item for a cultural experience in your city.
	\item for insurance covering.
	\item for investing in the stock market.
	\item for job training.
	\item for $\$500$.
	\item for spending on a present to someone you love.
	\item at your favorite restaurant.
\end{itemize}

\end{document}


